# æŠ¤ç†æ´»åŠ¨è¯†åˆ«æ·±åº¦å­¦ä¹ æ¨¡å‹ - PyTorchå®ç°

## é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®å®ç°äº†äº”ä¸ªå…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºåŸºäºä¼ æ„Ÿå™¨æ•°æ®çš„æŠ¤ç†æ´»åŠ¨è‡ªåŠ¨è¯†åˆ«ã€‚ä½¿ç”¨SONARæŠ¤ç†æ´»åŠ¨æ•°æ®é›†ï¼Œé€šè¿‡æ—¶é—´åºåˆ—åˆ†æå’Œå¤šç§ç¥ç»ç½‘ç»œæ¶æ„æ¥åˆ†ç±»20ç§ä¸åŒçš„æŠ¤ç†æ´»åŠ¨ã€‚

### æ ¸å¿ƒç‰¹æ€§
- ğŸ§  **5ä¸ªå…ˆè¿›æ¨¡å‹æ¶æ„**ï¼šåŸºçº¿CNN-LSTMã€ç›¸å…³æ„ŸçŸ¥CNNã€æ³¨æ„åŠ›LSTMã€ç‰¹å¾é€‰æ‹©ç½‘ç»œå’Œæ··åˆç½‘ç»œ
- ğŸ“Š **æ¶ˆèç ”ç©¶**ï¼šç³»ç»Ÿæ€§åˆ†æå„ç»„ä»¶å¯¹æ¨¡å‹æ€§èƒ½çš„è´¡çŒ®
- ğŸ”§ **è¿‡æ‹Ÿåˆé˜²æ­¢**ï¼šæ ‡ç­¾å¹³æ»‘ã€æƒé‡è¡°å‡ã€å­¦ä¹ ç‡è°ƒåº¦ã€æ¢¯åº¦è£å‰ª
- ğŸ“ˆ **ç»¼åˆè¯„ä¼°**ï¼šå‡†ç¡®ç‡ã€F1åˆ†æ•°ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€ROCæ›²çº¿ã€æ··æ·†çŸ©é˜µ
- ğŸ“ **å®Œæ•´æ—¥å¿—**ï¼šè‡ªåŠ¨ä¿å­˜è®­ç»ƒè¿‡ç¨‹å’Œå®éªŒç»“æœ

## é—®é¢˜å…¬å¼åŒ– (Problem Formulation)

### æŠ¤ç†æ´»åŠ¨è¯†åˆ«é—®é¢˜çš„æ•°å­¦å®šä¹‰

æŠ¤ç†æ´»åŠ¨è¯†åˆ«é—®é¢˜å¯ä»¥å½¢å¼åŒ–ä¸ºä¸€ä¸ªå¤šå˜é‡æ—¶é—´åºåˆ—åˆ†ç±»ä»»åŠ¡ã€‚è®¾æƒ¯æ€§ä¼ æ„Ÿå™¨ç³»ç»Ÿåœ¨æ—¶é—´ $t$ äº§ç”Ÿ $D$ ç»´ç‰¹å¾å‘é‡ $\mathbf{x}_t \in \mathbb{R}^D$ï¼Œå…¶ä¸­ $D = 70$ è¡¨ç¤ºä»5ä¸ªèº«ä½“ä¼ æ„Ÿå™¨è·å¾—çš„å¤šæ¨¡æ€ç‰¹å¾ã€‚

**é—®é¢˜å®šä¹‰**ï¼šç»™å®šé•¿åº¦ä¸º $T$ çš„æ—¶é—´åºåˆ—è§‚æµ‹çª—å£ $\mathbf{X} = [\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_T] \in \mathbb{R}^{T \times D}$ï¼Œå­¦ä¹ ä¸€ä¸ªæ˜ å°„å‡½æ•°ï¼š

$$f: \mathbb{R}^{T \times D} \rightarrow \mathbb{R}^C$$

å°†æ—¶é—´åºåˆ—çª—å£æ˜ å°„åˆ° $C$ ä¸ªæŠ¤ç†æ´»åŠ¨ç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå…¶ä¸­ $C = 20$ è¡¨ç¤ºä¸åŒçš„æŠ¤ç†æ´»åŠ¨ç±»å‹ã€‚

### ä¼ æ„Ÿå™¨æ•°æ®çš„ç‰©ç†ç»“æ„

å¤šæ¨¡æ€ä¼ æ„Ÿå™¨æ•°æ®å…·æœ‰æ˜ç¡®çš„ç‰©ç†ç»“æ„ï¼Œåæ˜ äººä½“è¿åŠ¨çš„ä¸åŒæ–¹é¢ï¼š

$$\mathbf{x}_t = \begin{bmatrix} 
\mathbf{q}_t^{(1)}, \mathbf{q}_t^{(2)}, ..., \mathbf{q}_t^{(S)} \\
\dot{\mathbf{q}}_t^{(1)}, \dot{\mathbf{q}}_t^{(2)}, ..., \dot{\mathbf{q}}_t^{(S)} \\
\mathbf{v}_t^{(1)}, \mathbf{v}_t^{(2)}, ..., \mathbf{v}_t^{(S)} \\
\mathbf{m}_t^{(1)}, \mathbf{m}_t^{(2)}, ..., \mathbf{m}_t^{(S)}
\end{bmatrix}$$

å…¶ä¸­ $S = 5$ è¡¨ç¤ºä¼ æ„Ÿå™¨æ•°é‡ï¼š

- $\mathbf{q}_t^{(s)} \in \mathbb{R}^4$ï¼šä¼ æ„Ÿå™¨ $s$ çš„å››å…ƒæ•°å§¿æ€è¡¨ç¤º
- $\dot{\mathbf{q}}_t^{(s)} \in \mathbb{R}^4$ï¼šå››å…ƒæ•°å¯¼æ•°ï¼ˆè§’é€Ÿåº¦ç›¸å…³ï¼‰
- $\mathbf{v}_t^{(s)} \in \mathbb{R}^3$ï¼šçº¿æ€§é€Ÿåº¦å’ŒåŠ é€Ÿåº¦åˆ†é‡
- $\mathbf{m}_t^{(s)} \in \mathbb{R}^3$ï¼šä¸‰è½´ç£åœºå¼ºåº¦æµ‹é‡

### æ—¶é—´çª—å£åŒ–ä¸æ•°æ®é¢„å¤„ç†

**æ—¶é—´çª—å£æ„é€ **ï¼šä¸ºä¿æŒæ—¶é—´ä¾èµ–æ€§ï¼Œé‡‡ç”¨å›ºå®šé•¿åº¦çš„éé‡å çª—å£ï¼š

$$\mathbf{W}_i = \{\mathbf{x}_{(i-1) \cdot \tau + 1}, \mathbf{x}_{(i-1) \cdot \tau + 2}, ..., \mathbf{x}_{i \cdot \tau}\}$$

å…¶ä¸­ $\tau = 20$ ä¸ºçª—å£å¤§å°ï¼Œ$i$ ä¸ºçª—å£ç´¢å¼•ã€‚

**æ ‡å‡†åŒ–å¤„ç†**ï¼šä¸ºç¡®ä¿æ•°å€¼ç¨³å®šæ€§ï¼Œå¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦è¿›è¡ŒZ-scoreæ ‡å‡†åŒ–ï¼š

$$\tilde{\mathbf{x}}_t^{(d)} = \frac{\mathbf{x}_t^{(d)} - \mu^{(d)}}{\sigma^{(d)}}$$

å…¶ä¸­ $\mu^{(d)}$ å’Œ $\sigma^{(d)}$ åˆ†åˆ«ä¸ºç¬¬ $d$ ç»´ç‰¹å¾åœ¨è®­ç»ƒé›†ä¸Šçš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚

### ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜

æŠ¤ç†æ´»åŠ¨æ•°æ®å‘ˆç°æ˜¾è‘—çš„ç±»åˆ«ä¸å¹³è¡¡ï¼Œå®šä¹‰ä¸å¹³è¡¡æ¯”ä¸ºï¼š

$$\rho = \frac{\max_{c \in \{1,...,C\}} |\mathcal{D}_c|}{\min_{c \in \{1,...,C\}} |\mathcal{D}_c|}$$

å…¶ä¸­ $|\mathcal{D}_c|$ è¡¨ç¤ºç±»åˆ« $c$ çš„æ ·æœ¬æ•°é‡ã€‚æ•°æ®é›†ä¸­ $\rho \approx 156.7$ï¼Œéœ€è¦é‡‡ç”¨åŠ æƒæŸå¤±å‡½æ•°ï¼š

$$\mathcal{L}_{weighted} = -\sum_{i=1}^N w_{y_i} \log p(y_i | \mathbf{X}_i)$$

å…¶ä¸­æƒé‡ $w_c = \frac{N}{C \cdot |\mathcal{D}_c|}$ ç”¨äºå¹³è¡¡ç±»åˆ«è´¡çŒ®ã€‚

### æ•°æ®æ³„éœ²é˜²èŒƒç­–ç•¥

**ä¸»ä½“çº§åˆ†å‰²**ï¼šä¸ºé˜²æ­¢æ•°æ®æ³„éœ²ï¼Œä¸¥æ ¼æŒ‰ä¸»ä½“ï¼ˆå—è¯•è€…ï¼‰è¿›è¡Œæ•°æ®åˆ†å‰²ï¼š

$$\mathcal{S} = \mathcal{S}_{train} \cup \mathcal{S}_{val} \cup \mathcal{S}_{test}, \quad \mathcal{S}_{train} \cap \mathcal{S}_{val} \cap \mathcal{S}_{test} = \emptyset$$

å…¶ä¸­ $\mathcal{S}$ è¡¨ç¤ºæ‰€æœ‰å—è¯•è€…é›†åˆï¼Œç¡®ä¿ä»»ä½•å—è¯•è€…çš„æ•°æ®åªå‡ºç°åœ¨ä¸€ä¸ªå­é›†ä¸­ã€‚

**æ—¶é—´ç‹¬ç«‹æ€§**ï¼šé‡‡ç”¨éé‡å çª—å£ç¡®ä¿æ ·æœ¬é—´æ—¶é—´ç‹¬ç«‹ï¼š

$$\mathbf{W}_i \cap \mathbf{W}_j = \emptyset, \quad \forall i \neq j$$

### ä¼˜åŒ–ç›®æ ‡ä¸æŸå¤±å‡½æ•°

**ä¸»è¦ç›®æ ‡**ï¼šæœ€å°åŒ–é¢„æµ‹é”™è¯¯çš„æœŸæœ›é£é™©ï¼š

$$\mathcal{R}(f) = \mathbb{E}_{(\mathbf{X}, y) \sim \mathcal{D}} [\ell(f(\mathbf{X}), y)]$$

å…¶ä¸­ $\ell$ ä¸ºæŸå¤±å‡½æ•°ï¼Œ$\mathcal{D}$ ä¸ºçœŸå®æ•°æ®åˆ†å¸ƒã€‚

**å®é™…æŸå¤±**ï¼šç»“åˆæ ‡ç­¾å¹³æ»‘çš„äº¤å‰ç†µæŸå¤±ï¼š

$$\mathcal{L} = -\frac{1}{N} \sum_{i=1}^N \sum_{c=1}^C y_{i,c}^{smooth} \log \hat{y}_{i,c}$$

å…¶ä¸­å¹³æ»‘æ ‡ç­¾å®šä¹‰ä¸ºï¼š

$$y_{i,c}^{smooth} = (1-\alpha) y_{i,c} + \frac{\alpha}{C}$$

å¹³æ»‘å‚æ•° $\alpha = 0.1$ ç”¨äºæé«˜æ³›åŒ–èƒ½åŠ›ã€‚

### è¯„ä¼°æŒ‡æ ‡

**ä¸»è¦æŒ‡æ ‡**ï¼šå¤šç±»åˆ†ç±»å‡†ç¡®ç‡

$$\text{Accuracy} = \frac{1}{N_{test}} \sum_{i=1}^{N_{test}} \mathbb{I}[\arg\max_c \hat{y}_{i,c} = y_i]$$

**è¾…åŠ©æŒ‡æ ‡**ï¼šåŠ æƒF1åˆ†æ•°ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ä»¥åŠç±»åˆ«çº§æ€§èƒ½åˆ†æã€‚

## æ•°æ®é›†ä¿¡æ¯

### SONARæŠ¤ç†æ´»åŠ¨æ•°æ®é›†
- **æ•°æ®æ¥æº**ï¼šçœŸå®æŠ¤ç†ç¯å¢ƒä¸­çš„ä¼ æ„Ÿå™¨æ•°æ®
- **ç‰¹å¾ç»´åº¦**ï¼š70ç»´ä¼ æ„Ÿå™¨ç‰¹å¾ï¼ˆå››å…ƒæ•°ã€é€Ÿåº¦ã€ç£åœºç­‰ï¼‰
- **æ´»åŠ¨ç±»åˆ«**ï¼š20ç§æŠ¤ç†æ´»åŠ¨ï¼ˆæ¢è¡£æœã€åºŠä¸Šæ´—æ¼±ã€å¨æˆ¿å‡†å¤‡ç­‰ï¼‰
- **è¢«è¯•æ•°é‡**ï¼š13åè¢«è¯•
- **æ—¶é—´çª—å£**ï¼š20ä¸ªæ—¶é—´æ­¥é•¿ï¼Œéé‡å çª—å£
- **æ•°æ®åˆ†å‰²**ï¼šæŒ‰è¢«è¯•åˆ†å‰²ï¼ˆè®­ç»ƒ70%ï¼ŒéªŒè¯15%ï¼Œæµ‹è¯•15%ï¼‰

## ç†è®ºåŸºç¡€ä¸æ–¹æ³•è®º

### é—®é¢˜å½¢å¼åŒ–å®šä¹‰

ç»™å®šå¤šå˜é‡æ—¶é—´åºåˆ—æ•°æ® $\mathbf{X} = \{x_1, x_2, ..., x_T\}$ï¼Œå…¶ä¸­ $x_t \in \mathbb{R}^d$ è¡¨ç¤ºç¬¬ $t$ ä¸ªæ—¶é—´æ­¥çš„ $d$ ç»´ä¼ æ„Ÿå™¨ç‰¹å¾å‘é‡ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å­¦ä¹ ä¸€ä¸ªæ˜ å°„å‡½æ•° $f: \mathbb{R}^{T \times d} \rightarrow \mathbb{R}^C$ï¼Œå°†æ—¶é—´åºåˆ—çª—å£æ˜ å°„åˆ° $C$ ä¸ªæŠ¤ç†æ´»åŠ¨ç±»åˆ«ä¸­çš„ä¸€ä¸ªã€‚

### æ•°æ®è¡¨ç¤ºä¸é¢„å¤„ç†

#### ç‰¹å¾ç©ºé—´åˆ†è§£
åŸºäºä¼ æ„Ÿå™¨çš„ç‰©ç†ç‰¹æ€§ï¼Œæˆ‘ä»¬å°†70ç»´ç‰¹å¾å‘é‡åˆ†è§£ä¸ºå››ä¸ªè¯­ä¹‰ç»„ï¼š

$$\mathbf{x}_t = [\mathbf{q}_t; \mathbf{\dot{q}}_t; \mathbf{v}_t; \mathbf{m}_t]$$

å…¶ä¸­ï¼š
- $\mathbf{q}_t \in \mathbb{R}^{12}$ï¼šå››å…ƒæ•°ç‰¹å¾ï¼ˆå§¿æ€ä¿¡æ¯ï¼‰
- $\mathbf{\dot{q}}_t \in \mathbb{R}^{12}$ï¼šå››å…ƒæ•°å¯¼æ•°ï¼ˆå§¿æ€å˜åŒ–ç‡ï¼‰
- $\mathbf{v}_t \in \mathbb{R}^{24}$ï¼šé€Ÿåº¦ç‰¹å¾ï¼ˆè¿åŠ¨ä¿¡æ¯ï¼‰
- $\mathbf{m}_t \in \mathbb{R}^{22}$ï¼šç£åœºç‰¹å¾ï¼ˆæ–¹å‘ä¿¡æ¯ï¼‰

#### æ—¶é—´çª—å£æ„é€ 
é‡‡ç”¨éé‡å æ»‘åŠ¨çª—å£ç­–ç•¥ï¼Œæ¯ä¸ªçª—å£åŒ…å« $W=20$ ä¸ªè¿ç»­æ—¶é—´æ­¥ï¼š

$$\mathbf{X}^{(i)} = [x_{(i-1) \cdot W + 1}, x_{(i-1) \cdot W + 2}, ..., x_{i \cdot W}]$$

ä¸ºç¡®ä¿æ ‡ç­¾ä¸€è‡´æ€§ï¼Œä»…ä¿ç•™æ´»åŠ¨æ ‡ç­¾å®Œå…¨ç›¸åŒçš„çª—å£ã€‚

## æ¨¡å‹æ¶æ„è¯¦è§£

### 1. Baseline CNN-LSTMï¼šæ··åˆæ—¶ç©ºç‰¹å¾å­¦ä¹ 

#### è®¾è®¡åŠ¨æœº
æŠ¤ç†æ´»åŠ¨å…·æœ‰æ˜æ˜¾çš„æ—¶ç©ºåŒé‡ç‰¹æ€§ï¼šå±€éƒ¨æ—¶é—´æ¨¡å¼ï¼ˆå¦‚æ‰‹éƒ¨åŠ¨ä½œçš„ç¬æ—¶ç‰¹å¾ï¼‰å’Œå…¨å±€æ—¶åºä¾èµ–ï¼ˆå¦‚å®Œæ•´æ´»åŠ¨çš„æ—¶é—´æ¼”åŒ–ï¼‰ã€‚CNN-LSTMæ¶æ„é€šè¿‡åˆ†å±‚ç‰¹å¾æå–æ¥æ•è·è¿™ä¸¤ç§ç‰¹æ€§ã€‚

#### æ•°å­¦å»ºæ¨¡

**1Då·ç§¯å±‚**ï¼š
$$\mathbf{h}^{(1)}_t = \sigma(W_1 * \mathbf{x}_{t:t+k-1} + b_1)$$

å…¶ä¸­ $*$ è¡¨ç¤ºä¸€ç»´å·ç§¯æ“ä½œï¼Œ$k$ ä¸ºå·ç§¯æ ¸å¤§å°ï¼Œ$\sigma$ ä¸ºæ¿€æ´»å‡½æ•°ã€‚

**LSTMå±‚**ï¼š
$$\begin{aligned}
\mathbf{f}_t &= \sigma_g(W_f \mathbf{h}^{(2)}_t + U_f \mathbf{h}_{t-1} + b_f) \\
\mathbf{i}_t &= \sigma_g(W_i \mathbf{h}^{(2)}_t + U_i \mathbf{h}_{t-1} + b_i) \\
\mathbf{o}_t &= \sigma_g(W_o \mathbf{h}^{(2)}_t + U_o \mathbf{h}_{t-1} + b_o) \\
\tilde{\mathbf{c}}_t &= \sigma_h(W_c \mathbf{h}^{(2)}_t + U_c \mathbf{h}_{t-1} + b_c) \\
\mathbf{c}_t &= \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{c}}_t \\
\mathbf{h}_t &= \mathbf{o}_t \odot \sigma_h(\mathbf{c}_t)
\end{aligned}$$

**åŒå‘LSTM**ï¼š
$$\overrightarrow{\mathbf{h}}_t = \text{LSTM}(\mathbf{h}^{(2)}_t, \overrightarrow{\mathbf{h}}_{t-1})$$
$$\overleftarrow{\mathbf{h}}_t = \text{LSTM}(\mathbf{h}^{(2)}_t, \overleftarrow{\mathbf{h}}_{t+1})$$
$$\mathbf{h}_t^{\text{bi}} = [\overrightarrow{\mathbf{h}}_t; \overleftarrow{\mathbf{h}}_t]$$

#### è¯¦ç»†æ¶æ„
```python
æ¶æ„ç»„æˆï¼š
â”œâ”€â”€ 1Då·ç§¯å±‚ (input_size=70, filters=64, kernel=3, stride=1)
â”‚   â”œâ”€â”€ BatchNorm1d(64)
â”‚   â”œâ”€â”€ ReLUæ¿€æ´»
â”‚   â””â”€â”€ Dropout(0.3)
â”œâ”€â”€ 1Då·ç§¯å±‚ (input_size=64, filters=128, kernel=3, stride=1)
â”‚   â”œâ”€â”€ BatchNorm1d(128)
â”‚   â”œâ”€â”€ ReLUæ¿€æ´»
â”‚   â””â”€â”€ Dropout(0.3)
â”œâ”€â”€ åŒå‘LSTMå±‚ (input_size=128, hidden_size=64)
â”‚   â””â”€â”€ è¾“å‡ºç»´åº¦ï¼š128 (64Ã—2)
â”œâ”€â”€ Dropout(0.5)
â”œâ”€â”€ å…¨è¿æ¥å±‚ (128 â†’ num_classes)
â””â”€â”€ Softmaxæ¿€æ´»
```

#### å…³é”®åˆ›æ–°ç‚¹
1. **å±‚æ¬¡åŒ–ç‰¹å¾æå–**ï¼šCNNæ•è·å±€éƒ¨æ—¶é—´æ¨¡å¼ï¼ŒLSTMå»ºæ¨¡é•¿æœŸä¾èµ–
2. **åŒå‘ä¸Šä¸‹æ–‡**ï¼šåŒå‘LSTMåˆ©ç”¨æœªæ¥å’Œè¿‡å»ä¿¡æ¯
3. **æ­£åˆ™åŒ–ç­–ç•¥**ï¼šæ‰¹å½’ä¸€åŒ–å’ŒDropouté˜²æ­¢è¿‡æ‹Ÿåˆ

#### æ•°æ®åˆ©ç”¨æ–¹å¼
- ç›´æ¥å¤„ç†åŸå§‹70ç»´ç‰¹å¾
- æ—¶é—´çª—å£å¤§å°ï¼š20
- ä¿æŒç‰¹å¾é—´çš„åŸå§‹å…³ç³»

### 2. Correlation-Aware CNNï¼šç‰©ç†çº¦æŸçš„ç‰¹å¾å­¦ä¹ 

#### è®¾è®¡åŠ¨æœº
ä¼ ç»ŸCNNå°†æ‰€æœ‰ç‰¹å¾è§†ä¸ºåŒè´¨ï¼Œå¿½ç•¥äº†ä¼ æ„Ÿå™¨æ•°æ®çš„ç‰©ç†ç»“æ„ã€‚ä¸åŒä¼ æ„Ÿå™¨ç»„å…·æœ‰ä¸åŒçš„ç‰©ç†æ„ä¹‰å’Œç›¸å…³æ€§æ¨¡å¼ã€‚è¯¥æ¶æ„åŸºäºä¼ æ„Ÿå™¨çš„ç‰©ç†ç‰¹æ€§è¿›è¡Œåˆ†ç»„å¤„ç†ï¼Œå­¦ä¹ ç»„å†…ç›¸å…³æ€§å’Œç»„é—´ç›¸äº’ä½œç”¨ã€‚

#### ç†è®ºåŸºç¡€
**ç‰¹å¾åˆ†ç»„å‡è®¾**ï¼šå‡è®¾å­˜åœ¨ç‰¹å¾åˆ†ç»„ $\mathcal{G} = \{G_1, G_2, G_3, G_4\}$ï¼Œå…¶ä¸­æ¯ç»„å†…çš„ç‰¹å¾å…·æœ‰æ›´å¼ºçš„ç›¸å…³æ€§ï¼Œç»„é—´å­˜åœ¨å¯å­¦ä¹ çš„äº¤äº’æ¨¡å¼ã€‚

**ç»„å†…ç›¸å…³æ€§å»ºæ¨¡**ï¼š
å¯¹äºç¬¬ $k$ ç»„ç‰¹å¾ $\mathbf{x}^{(k)}_t \in \mathbb{R}^{d_k}$ï¼Œåº”ç”¨ä¸“é—¨çš„å·ç§¯æ ¸ï¼š
$$\mathbf{h}^{(k)}_t = \sigma(W^{(k)} * \mathbf{x}^{(k)}_{t:t+w-1} + b^{(k)})$$

**ç»„é—´ç›¸å…³æ€§å­¦ä¹ **ï¼š
å®šä¹‰ç›¸å…³æ€§å‡½æ•° $\rho: \mathbb{R}^{d_i} \times \mathbb{R}^{d_j} \rightarrow \mathbb{R}^{d_{ij}}$ï¼š
$$\mathbf{c}_{ij} = \rho(\mathbf{h}^{(i)}, \mathbf{h}^{(j)}) = \frac{\mathbf{h}^{(i)} \odot \mathbf{h}^{(j)}}{\|\mathbf{h}^{(i)}\|_2 \|\mathbf{h}^{(j)}\|_2}$$

**ç‰¹å¾èåˆ**ï¼š
$$\mathbf{h}_{\text{fused}} = \text{Concat}([\mathbf{h}^{(1)}, \mathbf{h}^{(2)}, \mathbf{h}^{(3)}, \mathbf{h}^{(4)}, \mathbf{c}_{12}, \mathbf{c}_{13}, ..., \mathbf{c}_{34}])$$

#### è¯¦ç»†æ¶æ„
```python
åˆ†ç»„å®šä¹‰ï¼š
â”œâ”€â”€ å››å…ƒæ•°ç»„ (Gâ‚): [0:12]   - å§¿æ€å››å…ƒæ•° w,x,y,z
â”œâ”€â”€ å››å…ƒæ•°å¯¼æ•°ç»„ (Gâ‚‚): [12:24] - å§¿æ€å˜åŒ–ç‡
â”œâ”€â”€ é€Ÿåº¦ç»„ (Gâ‚ƒ): [24:48]    - ä¸‰è½´é€Ÿåº¦å’ŒåŠ é€Ÿåº¦
â””â”€â”€ ç£åœºç»„ (Gâ‚„): [48:70]    - ç£åŠ›è®¡æ•°æ®

æ¯ç»„å¤„ç†æµç¨‹ï¼š
â”œâ”€â”€ 1Dåˆ†ç»„å·ç§¯ (group_conv1d, filters=32, kernel=3)
â”œâ”€â”€ BatchNorm1d + ReLU
â”œâ”€â”€ å…¨å±€å¹³å‡æ± åŒ– (AdaptiveAvgPool1d)
â””â”€â”€ è¾“å‡ºï¼šå„ç»„ç‰¹å¾è¡¨ç¤º

ç›¸å…³æ€§è®¡ç®—ï¼š
â”œâ”€â”€ L2å½’ä¸€åŒ–ï¼šhâ½â±â¾_norm = hâ½â±â¾ / ||hâ½â±â¾||â‚‚
â”œâ”€â”€ å…ƒç´ ä¹˜ç§¯ï¼šc_ij = hâ½â±â¾_norm âŠ™ hâ½Ê²â¾_norm
â””â”€â”€ ç›¸å…³æ€§æƒé‡ï¼šÎ±_ij (å¯å­¦ä¹ å‚æ•°)

æœ€ç»ˆèåˆï¼š
â”œâ”€â”€ ç‰¹å¾æ‹¼æ¥ï¼š[hâ½Â¹â¾, hâ½Â²â¾, hâ½Â³â¾, hâ½â´â¾, câ‚â‚‚, câ‚â‚ƒ, câ‚â‚„, câ‚‚â‚ƒ, câ‚‚â‚„, câ‚ƒâ‚„]
â”œâ”€â”€ å…¨è¿æ¥å±‚ (input_dim: 4Ã—32 + 6Ã—32 = 320)
â””â”€â”€ åˆ†ç±»è¾“å‡º
```

#### æ•°å­¦æ¨å¯¼

**åˆ†ç»„å·ç§¯çš„ä¼˜åŠ¿**ï¼š
å‚æ•°å‡å°‘é‡ï¼š
$$\text{Reduction} = 1 - \frac{\sum_{k=1}^{4} d_k \cdot f_k}{d \cdot f}$$

å…¶ä¸­ $d_k$ ä¸ºç¬¬ $k$ ç»„çš„ç‰¹å¾æ•°ï¼Œ$f_k$ ä¸ºå¯¹åº”çš„æ»¤æ³¢å™¨æ•°ã€‚

**ç›¸å…³æ€§åº¦é‡çš„ç†è®ºä¾æ®**ï¼š
ä½™å¼¦ç›¸ä¼¼åº¦å˜ç§ï¼š
$$\text{sim}(\mathbf{u}, \mathbf{v}) = \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|} = \cos(\theta)$$

å…ƒç´ çº§ä¹˜ç§¯æ•è·ç‰¹å¾å¯¹åº”å…³ç³»ï¼š
$$\mathbf{c} = \mathbf{u}_{\text{norm}} \odot \mathbf{v}_{\text{norm}}$$

#### å…³é”®åˆ›æ–°ç‚¹
1. **ç‰©ç†æ„ŸçŸ¥åˆ†ç»„**ï¼šåŸºäºä¼ æ„Ÿå™¨ç‰©ç†æ„ä¹‰çš„ç‰¹å¾åˆ†ç»„
2. **ç›¸å…³æ€§æ˜¾å¼å»ºæ¨¡**ï¼šé€šè¿‡å¯å­¦ä¹ çš„ç›¸å…³æ€§å‡½æ•°æ•è·ç»„é—´å…³ç³»
3. **å‚æ•°æ•ˆç‡**ï¼šåˆ†ç»„å·ç§¯æ˜¾è‘—å‡å°‘å‚æ•°æ•°é‡
4. **é¢†åŸŸçŸ¥è¯†èå…¥**ï¼šå°†ä¼ æ„Ÿå™¨é¢†åŸŸçŸ¥è¯†ç¼–ç åˆ°ç½‘ç»œç»“æ„ä¸­

### 3. Attention LSTMï¼šè‡ªé€‚åº”æ—¶åºæ³¨æ„åŠ›æœºåˆ¶

#### è®¾è®¡åŠ¨æœº
æŠ¤ç†æ´»åŠ¨å…·æœ‰ä¸åŒçš„æ—¶é—´é‡è¦æ€§åˆ†å¸ƒï¼Œä¼ ç»ŸLSTMå¹³ç­‰å¯¹å¾…æ‰€æœ‰æ—¶é—´æ­¥ã€‚è¯¥æ¶æ„é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶åŠ¨æ€è¯†åˆ«å…³é”®æ—¶é—´æ®µï¼Œæé«˜å¯¹é‡è¦åŠ¨ä½œçš„å…³æ³¨åº¦ã€‚

#### ç†è®ºåŸºç¡€
**æ³¨æ„åŠ›å‡è®¾**ï¼šåœ¨æ—¶é—´åºåˆ— $\{\mathbf{h}_1, \mathbf{h}_2, ..., \mathbf{h}_T\}$ ä¸­ï¼Œä¸åŒæ—¶é—´æ­¥å¯¹æœ€ç»ˆé¢„æµ‹çš„è´¡çŒ®ä¸åŒï¼Œå­˜åœ¨å¯å­¦ä¹ çš„é‡è¦æ€§æƒé‡åˆ†å¸ƒã€‚

**å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼š
å®šä¹‰æŸ¥è¯¢ã€é”®ã€å€¼çŸ©é˜µï¼š
$$\mathbf{Q} = \mathbf{H}W_Q, \quad \mathbf{K} = \mathbf{H}W_K, \quad \mathbf{V} = \mathbf{H}W_V$$

å…¶ä¸­ $\mathbf{H} = [\mathbf{h}_1; \mathbf{h}_2; ...; \mathbf{h}_T] \in \mathbb{R}^{T \times d}$

**æ³¨æ„åŠ›æƒé‡è®¡ç®—**ï¼š
$$\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}$$

**å¤šå¤´æœºåˆ¶**ï¼š
$$\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W_O$$

å…¶ä¸­ï¼š
$$\text{head}_i = \text{Attention}(\mathbf{Q}W_Q^i, \mathbf{K}W_K^i, \mathbf{V}W_V^i)$$

#### è¯¦ç»†æ¶æ„
```python
è¾“å…¥å¤„ç†ï¼š
â”œâ”€â”€ è¾“å…¥ï¼š[batch_size, seq_len=20, input_size=70]
â”œâ”€â”€ çº¿æ€§æŠ•å½±å±‚ï¼š70 â†’ 128
â””â”€â”€ ä½ç½®ç¼–ç ï¼ˆå¯é€‰ï¼‰

åŒå‘LSTMç¼–ç å™¨ï¼š
â”œâ”€â”€ LSTM(input_size=128, hidden_size=64, bidirectional=True)
â”œâ”€â”€ è¾“å‡ºï¼š[batch_size, seq_len, 128] (64Ã—2)
â””â”€â”€ Dropout(0.3)

å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼š
â”œâ”€â”€ å¤´æ•°ï¼šh=8
â”œâ”€â”€ æ¯å¤´ç»´åº¦ï¼šd_k = d_v = 128/8 = 16
â”œâ”€â”€ Query/Key/ValueæŠ•å½±ï¼š
â”‚   â”œâ”€â”€ W_Q âˆˆ â„^(128Ã—128)
â”‚   â”œâ”€â”€ W_K âˆˆ â„^(128Ã—128)  
â”‚   â””â”€â”€ W_V âˆˆ â„^(128Ã—128)
â”œâ”€â”€ ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼š
â”‚   â””â”€â”€ Î±_ij = softmax(Q_iÂ·K_j^T / âˆš16)
â””â”€â”€ è¾“å‡ºæŠ•å½±ï¼šW_O âˆˆ â„^(128Ã—128)

æ®‹å·®è¿æ¥ä¸å½’ä¸€åŒ–ï¼š
â”œâ”€â”€ æ®‹å·®è¿æ¥ï¼šoutput = input + attention_output
â”œâ”€â”€ Layer Normalization
â””â”€â”€ å‰é¦ˆç½‘ç»œï¼š128 â†’ 256 â†’ 128

å…¨å±€æ± åŒ–ä¸åˆ†ç±»ï¼š
â”œâ”€â”€ å…¨å±€å¹³å‡æ± åŒ–ï¼š[batch_size, seq_len, 128] â†’ [batch_size, 128]
â”œâ”€â”€ å…¨è¿æ¥å±‚ï¼š128 â†’ num_classes
â””â”€â”€ Softmaxæ¿€æ´»
```

#### æ•°å­¦æ¨å¯¼

**æ³¨æ„åŠ›æƒé‡çš„æ„ä¹‰**ï¼š
æ³¨æ„åŠ›æƒé‡ $\alpha_{ij}$ è¡¨ç¤ºä½ç½® $i$ å¯¹ä½ç½® $j$ çš„å…³æ³¨ç¨‹åº¦ï¼š
$$\alpha_{ij} = \frac{\exp(\text{score}(\mathbf{h}_i, \mathbf{h}_j))}{\sum_{k=1}^T \exp(\text{score}(\mathbf{h}_i, \mathbf{h}_k))}$$

**ç¼©æ”¾å› å­çš„ç†è®ºä¾æ®**ï¼š
å½“ $d_k$ è¾ƒå¤§æ—¶ï¼Œç‚¹ç§¯å€¼å¯èƒ½å¾ˆå¤§ï¼Œä½¿softmaxå‡½æ•°è¿›å…¥é¥±å’ŒåŒºåŸŸã€‚ç¼©æ”¾å› å­ $\frac{1}{\sqrt{d_k}}$ ç¡®ä¿æ¢¯åº¦ç¨³å®šæ€§ï¼š
$$\text{Var}(\mathbf{q} \cdot \mathbf{k}) = d_k \cdot \text{Var}(q_i) \cdot \text{Var}(k_i) = d_k$$

ç¼©æ”¾åï¼š$\text{Var}\left(\frac{\mathbf{q} \cdot \mathbf{k}}{\sqrt{d_k}}\right) = 1$

**å¤šå¤´æ³¨æ„åŠ›çš„ç†è®ºä¼˜åŠ¿**ï¼š
ä¸åŒçš„æ³¨æ„åŠ›å¤´å¯ä»¥å…³æ³¨ä¸åŒçš„å…³ç³»æ¨¡å¼ï¼š
- Head 1: çŸ­æœŸä¾èµ–ï¼ˆç›¸é‚»æ—¶é—´æ­¥ï¼‰
- Head 2: ä¸­æœŸæ¨¡å¼ï¼ˆå±€éƒ¨å³°å€¼ï¼‰
- Head 3: é•¿æœŸè¶‹åŠ¿ï¼ˆå…¨å±€æ¨¡å¼ï¼‰

**æ—¶é—´å¤æ‚åº¦åˆ†æ**ï¼š
- è‡ªæ³¨æ„åŠ›ï¼š$O(T^2 \cdot d)$
- LSTMï¼š$O(T \cdot d^2)$
- æ€»å¤æ‚åº¦ï¼š$O(T^2 \cdot d + T \cdot d^2)$

å¯¹äº $T=20, d=128$ï¼šè‡ªæ³¨æ„åŠ›å ä¸»å¯¼åœ°ä½

#### å…³é”®åˆ›æ–°ç‚¹
1. **åŠ¨æ€æ³¨æ„åŠ›åˆ†é…**ï¼šè‡ªé€‚åº”è¯†åˆ«é‡è¦æ—¶é—´æ®µ
2. **å¤šç»´åº¦å…³ç³»å»ºæ¨¡**ï¼šå¤šå¤´æœºåˆ¶æ•è·ä¸åŒç±»å‹çš„æ—¶åºæ¨¡å¼
3. **é•¿è·ç¦»ä¾èµ–**ï¼šå…‹æœLSTMçš„é•¿æœŸä¾èµ–é—®é¢˜
4. **å¹¶è¡Œè®¡ç®—**ï¼šæ³¨æ„åŠ›æœºåˆ¶æ”¯æŒå¹¶è¡ŒåŒ–ï¼Œæé«˜è®­ç»ƒæ•ˆç‡

### 4. Feature-Selective Netï¼šè‡ªé€‚åº”ç‰¹å¾é€‰æ‹©æœºåˆ¶

#### è®¾è®¡åŠ¨æœº
åœ¨70ç»´ä¼ æ„Ÿå™¨ç‰¹å¾ä¸­ï¼Œå¹¶éæ‰€æœ‰ç‰¹å¾å¯¹æ¯ä¸ªæ´»åŠ¨éƒ½åŒç­‰é‡è¦ã€‚è¯¥æ¶æ„é€šè¿‡å¯å­¦ä¹ çš„ç‰¹å¾é€‰æ‹©é—¨æ§æœºåˆ¶ï¼ŒåŠ¨æ€è¯†åˆ«å’Œå¼ºè°ƒå¯¹å½“å‰æ ·æœ¬æœ€ç›¸å…³çš„ç‰¹å¾å­é›†ã€‚

#### ç†è®ºåŸºç¡€
**ç‰¹å¾é‡è¦æ€§å‡è®¾**ï¼šå¯¹äºä¸åŒçš„æŠ¤ç†æ´»åŠ¨ï¼Œç‰¹å¾çš„é‡è¦æ€§å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚å®šä¹‰ç‰¹å¾é‡è¦æ€§å‘é‡ $\mathbf{g} \in [0,1]^d$ï¼Œå…¶ä¸­ $g_i$ è¡¨ç¤ºç¬¬ $i$ ä¸ªç‰¹å¾çš„é‡è¦æ€§æƒé‡ã€‚

**é—¨æ§æœºåˆ¶æ•°å­¦å®šä¹‰**ï¼š
$$\mathbf{g} = \sigma(\mathbf{W}_g \mathbf{x} + \mathbf{b}_g)$$

å…¶ä¸­ $\sigma$ ä¸ºSigmoidå‡½æ•°ï¼Œç¡®ä¿é—¨æ§æƒé‡åœ¨ $[0,1]$ èŒƒå›´å†…ã€‚

**ç‰¹å¾é€‰æ‹©æ“ä½œ**ï¼š
$$\mathbf{x}_{\text{selected}} = \mathbf{g} \odot \mathbf{x}$$

å…¶ä¸­ $\odot$ è¡¨ç¤ºå…ƒç´ çº§ä¹˜æ³•ã€‚

#### è¯¦ç»†æ¶æ„
```python
ç‰¹å¾é€‰æ‹©é—¨æ§æ¨¡å—ï¼š
â”œâ”€â”€ è¾“å…¥ï¼š[batch_size, seq_len=20, features=70]
â”œâ”€â”€ å…¨å±€å¹³å‡æ± åŒ–ï¼š[batch_size, seq_len, 70] â†’ [batch_size, 70]
â”œâ”€â”€ ç‰¹å¾é‡è¦æ€§ç½‘ç»œï¼š
â”‚   â”œâ”€â”€ å…¨è¿æ¥å±‚1ï¼š70 â†’ 35 (ç‰¹å¾å‹ç¼©)
â”‚   â”œâ”€â”€ ReLUæ¿€æ´»
â”‚   â”œâ”€â”€ Dropout(0.3)
â”‚   â”œâ”€â”€ å…¨è¿æ¥å±‚2ï¼š35 â†’ 70 (ç‰¹å¾æ¢å¤)
â”‚   â””â”€â”€ Sigmoidæ¿€æ´» â†’ é—¨æ§æƒé‡ g âˆˆ [0,1]^70
â””â”€â”€ é—¨æ§æ“ä½œï¼šx_gated = g âŠ™ x (é€å…ƒç´ ç›¸ä¹˜)

ä¸»å¹²ç½‘ç»œï¼š
â”œâ”€â”€ è¾“å…¥ï¼šé—¨æ§åçš„ç‰¹å¾ [batch_size, seq_len, 70]
â”œâ”€â”€ 1Då·ç§¯å±‚1ï¼š(filters=64, kernel=3)
â”‚   â”œâ”€â”€ BatchNorm1d + ReLU
â”‚   â””â”€â”€ Dropout(0.3)
â”œâ”€â”€ 1Då·ç§¯å±‚2ï¼š(filters=128, kernel=3)
â”‚   â”œâ”€â”€ BatchNorm1d + ReLU
â”‚   â””â”€â”€ Dropout(0.3)
â”œâ”€â”€ å…¨å±€å¹³å‡æ± åŒ–ï¼š[batch_size, seq_len, 128] â†’ [batch_size, 128]
â”œâ”€â”€ å…¨è¿æ¥å±‚ï¼š128 â†’ num_classes
â””â”€â”€ Softmaxæ¿€æ´»
```

#### æ•°å­¦æ¨å¯¼

**é—¨æ§å‡½æ•°çš„æ€§è´¨åˆ†æ**ï¼š
Sigmoidå‡½æ•°çš„å¯¼æ•°ï¼š
$$\frac{\partial \sigma(x)}{\partial x} = \sigma(x)(1 - \sigma(x))$$

å½“ $\sigma(x) \to 0$ æˆ– $\sigma(x) \to 1$ æ—¶ï¼Œæ¢¯åº¦è¶‹è¿‘äº0ï¼Œå®ç°"ç¡¬"é€‰æ‹©æ•ˆæœã€‚

**ç‰¹å¾é€‰æ‹©çš„ä¿¡æ¯è®ºè§£é‡Š**ï¼š
å®šä¹‰é€‰æ‹©åçš„ä¿¡æ¯ç†µï¼š
$$H(\mathbf{x}_{\text{selected}}) = -\sum_{i=1}^d p(x_i) \log p(x_i)$$

å…¶ä¸­ $p(x_i) = \frac{g_i |x_i|}{\sum_{j=1}^d g_j |x_j|}$

ç›®æ ‡æ˜¯æœ€å¤§åŒ–ç›¸å…³ç‰¹å¾çš„ä¿¡æ¯ç†µï¼ŒåŒæ—¶æœ€å°åŒ–æ— å…³ç‰¹å¾çš„è´¡çŒ®ã€‚

**é—¨æ§æƒé‡çš„æ­£åˆ™åŒ–**ï¼š
ä¸ºé˜²æ­¢è¿‡åº¦ç¨€ç–åŒ–ï¼Œå¼•å…¥L1æ­£åˆ™åŒ–é¡¹ï¼š
$$\mathcal{L}_{\text{reg}} = \lambda \sum_{i=1}^d |g_i|$$

æ€»æŸå¤±å‡½æ•°ï¼š
$$\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{CE}} + \mathcal{L}_{\text{reg}}$$

#### é—¨æ§æœºåˆ¶çš„ç†è®ºä¼˜åŠ¿

**1. è‡ªé€‚åº”æ€§**ï¼š
ä¸åŒæ ·æœ¬æ¿€æ´»ä¸åŒçš„ç‰¹å¾å­é›†ï¼š
$$\mathbf{g}^{(n)} = f_{\text{gate}}(\mathbf{x}^{(n)})$$

**2. å¯è§£é‡Šæ€§**ï¼š
é—¨æ§æƒé‡ç›´æ¥åæ˜ ç‰¹å¾é‡è¦æ€§ï¼Œä¾¿äºåˆ†æï¼š
$$\text{Importance}(f_i) = \mathbb{E}[g_i]$$

**3. è®¡ç®—æ•ˆç‡**ï¼š
é€šè¿‡ç‰¹å¾é€‰æ‹©å‡å°‘åç»­è®¡ç®—ï¼š
$$\text{Complexity}_{\text{reduced}} = \text{Complexity}_{\text{original}} \times \mathbb{E}[\|\mathbf{g}\|_1/d]$$

#### å…³é”®åˆ›æ–°ç‚¹
1. **åŠ¨æ€ç‰¹å¾é€‰æ‹©**ï¼šæ ¹æ®è¾“å…¥æ ·æœ¬è‡ªé€‚åº”è°ƒæ•´ç‰¹å¾æƒé‡
2. **ç«¯åˆ°ç«¯å­¦ä¹ **ï¼šé—¨æ§æƒé‡ä¸ä¸»åˆ†ç±»å™¨è”åˆä¼˜åŒ–
3. **å¯è§£é‡Šæ€§å¢å¼º**ï¼šæä¾›ç‰¹å¾é‡è¦æ€§çš„ç›´è§‚è§£é‡Š
4. **è®¡ç®—æ•ˆç‡**ï¼šå‡å°‘æ— å…³ç‰¹å¾çš„è®¡ç®—å¼€é”€
â”‚   â””â”€â”€ å…ƒç´ çº§åˆ«ç‰¹å¾é—¨æ§
â”œâ”€â”€ 1Då·ç§¯ç‰¹å¾æå–
â”œâ”€â”€ å…¨å±€å¹³å‡æ± åŒ–
â”œâ”€â”€ å…¨è¿æ¥åˆ†ç±»å™¨
â””â”€â”€ Softmaxè¾“å‡º
```

**ç‰¹ç‚¹**ï¼š
- è‡ªé€‚åº”ç‰¹å¾é€‰æ‹©æœºåˆ¶
- å‡å°‘å™ªå£°ç‰¹å¾å½±å“
- æé«˜æ¨¡å‹è§£é‡Šæ€§

### 5. HybridNetï¼šé›†æˆå¤šæ¨¡æ€å­¦ä¹ æ¶æ„

#### è®¾è®¡åŠ¨æœº
å•ä¸€æŠ€æœ¯å¾€å¾€åªèƒ½è§£å†³ç‰¹å®šé—®é¢˜ï¼Œè€ŒæŠ¤ç†æ´»åŠ¨è¯†åˆ«é¢ä¸´å¤šé‡æŒ‘æˆ˜ï¼šç‰¹å¾å†—ä½™ã€æ—¶åºå¤æ‚æ€§ã€ç‰©ç†çº¦æŸç­‰ã€‚HybridNeté€šè¿‡æ¨¡å—åŒ–è®¾è®¡é›†æˆä¸‰ç§äº’è¡¥æŠ€æœ¯ï¼Œå®ç°ååŒä¼˜åŒ–ã€‚

#### ç†è®ºåŸºç¡€
**é›†æˆå­¦ä¹ ç†è®º**ï¼šå‡è®¾å­˜åœ¨ä¸‰ä¸ªç‹¬ç«‹çš„ç‰¹å¾å˜æ¢å‡½æ•°ï¼š
- $f_{\text{fs}}: \mathbb{R}^d \rightarrow \mathbb{R}^d$ (ç‰¹å¾é€‰æ‹©)
- $f_{\text{ca}}: \mathbb{R}^d \rightarrow \mathbb{R}^{d'}$ (ç›¸å…³æ„ŸçŸ¥)  
- $f_{\text{ta}}: \mathbb{R}^{T \times d'} \rightarrow \mathbb{R}^{d''}$ (æ—¶é—´æ³¨æ„åŠ›)

**é›†æˆæ˜ å°„**ï¼š
$$\mathbf{h}_{\text{hybrid}} = f_{\text{ta}}(f_{\text{ca}}(f_{\text{fs}}(\mathbf{X})))$$

#### è¯¦ç»†æ¶æ„
```python
HybridNetå®Œæ•´æ¶æ„ï¼š

é˜¶æ®µ1ï¼šè‡ªé€‚åº”ç‰¹å¾é€‰æ‹©
â”œâ”€â”€ è¾“å…¥ï¼š[batch_size, seq_len=20, features=70]
â”œâ”€â”€ å…¨å±€ä¸Šä¸‹æ–‡æå–ï¼šGlobalAvgPool1d
â”œâ”€â”€ ç‰¹å¾é‡è¦æ€§ç½‘ç»œï¼š
â”‚   â”œâ”€â”€ FC1: 70 â†’ 35, ReLU, Dropout(0.3)
â”‚   â”œâ”€â”€ FC2: 35 â†’ 70, Sigmoid
â”‚   â””â”€â”€ è¾“å‡ºï¼šé—¨æ§æƒé‡ g âˆˆ [0,1]^70
â””â”€â”€ é—¨æ§æ“ä½œï¼šX_fs = g âŠ™ X

é˜¶æ®µ2ï¼šç‰©ç†æ„ŸçŸ¥ç›¸å…³æ€§å­¦ä¹ 
â”œâ”€â”€ è¾“å…¥ï¼šX_fs [batch_size, seq_len, 70]
â”œâ”€â”€ ç‰¹å¾åˆ†ç»„ï¼š
â”‚   â”œâ”€â”€ Gâ‚: å››å…ƒæ•° [0:12]
â”‚   â”œâ”€â”€ Gâ‚‚: å››å…ƒæ•°å¯¼æ•° [12:24]  
â”‚   â”œâ”€â”€ Gâ‚ƒ: é€Ÿåº¦ [24:48]
â”‚   â””â”€â”€ Gâ‚„: ç£åœº [48:70]
â”œâ”€â”€ åˆ†ç»„å·ç§¯å¤„ç†ï¼š
â”‚   â”œâ”€â”€ æ¯ç»„ï¼šConv1d(filters=32, kernel=3) + BatchNorm + ReLU
â”‚   â””â”€â”€ è¾“å‡ºï¼š4ä¸ªç»„ç‰¹å¾ hâ½Â¹â¾, hâ½Â²â¾, hâ½Â³â¾, hâ½â´â¾
â”œâ”€â”€ ç›¸å…³æ€§è®¡ç®—ï¼š
â”‚   â”œâ”€â”€ ç»„é—´ç›¸å…³æ€§ï¼šc_ij = corr(hâ½â±â¾, hâ½Ê²â¾) for iâ‰ j
â”‚   â””â”€â”€ æ€»å…±6ä¸ªç›¸å…³æ€§ç‰¹å¾
â””â”€â”€ ç‰¹å¾èåˆï¼šX_ca = Concat[hâ½Â¹â¾, hâ½Â²â¾, hâ½Â³â¾, hâ½â´â¾, câ‚â‚‚, câ‚â‚ƒ, câ‚â‚„, câ‚‚â‚ƒ, câ‚‚â‚„, câ‚ƒâ‚„]

é˜¶æ®µ3ï¼šæ—¶åºæ³¨æ„åŠ›å»ºæ¨¡
â”œâ”€â”€ è¾“å…¥ï¼šX_ca [batch_size, seq_len, 320] (4Ã—32 + 6Ã—32)
â”œâ”€â”€ åŒå‘LSTMç¼–ç ï¼š
â”‚   â”œâ”€â”€ LSTM(input_size=320, hidden_size=64, bidirectional=True)
â”‚   â””â”€â”€ è¾“å‡ºï¼š[batch_size, seq_len, 128]
â”œâ”€â”€ å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼š
â”‚   â”œâ”€â”€ å¤´æ•°ï¼š8, æ¯å¤´ç»´åº¦ï¼š16
â”‚   â”œâ”€â”€ Q,K,VæŠ•å½±ï¼š128 â†’ 128
â”‚   â”œâ”€â”€ ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼šsoftmax(QK^T/âˆš16)V
â”‚   â””â”€â”€ è¾“å‡ºæŠ•å½±ï¼š128 â†’ 128
â”œâ”€â”€ æ®‹å·®è¿æ¥ï¼šoutput = LSTM_out + Attention_out
â”œâ”€â”€ Layer Normalization
â””â”€â”€ å‰é¦ˆç½‘ç»œï¼š128 â†’ 256 â†’ 128

æœ€ç»ˆåˆ†ç±»ï¼š
â”œâ”€â”€ å…¨å±€å¹³å‡æ± åŒ–ï¼š[batch_size, seq_len, 128] â†’ [batch_size, 128]
â”œâ”€â”€ åˆ†ç±»å™¨ï¼š
â”‚   â”œâ”€â”€ FC1: 128 â†’ 64, ReLU, Dropout(0.5)
â”‚   â”œâ”€â”€ FC2: 64 â†’ num_classes
â”‚   â””â”€â”€ Softmaxæ¿€æ´»
â””â”€â”€ è¾“å‡ºï¼šç±»åˆ«æ¦‚ç‡åˆ†å¸ƒ
```

#### æ•°å­¦å»ºæ¨¡

**æ¨¡å—åŒ–é›†æˆçš„æ•°å­¦è¡¨ç¤º**ï¼š
å®šä¹‰å¯é…ç½®çš„æ¨¡å—é€‰æ‹©å™¨ï¼š
$$\mathcal{M} = \{\alpha_{\text{fs}}, \alpha_{\text{ca}}, \alpha_{\text{ta}}\} \in \{0,1\}^3$$

**æ¡ä»¶æ‰§è¡Œ**ï¼š
```math
\begin{align}
\mathbf{X}_1 &= \begin{cases}
f_{\text{fs}}(\mathbf{X}) & \text{if } \alpha_{\text{fs}} = 1 \\
\mathbf{X} & \text{otherwise}
\end{cases} \\
\mathbf{X}_2 &= \begin{cases}
f_{\text{ca}}(\mathbf{X}_1) & \text{if } \alpha_{\text{ca}} = 1 \\
\mathbf{X}_1 & \text{otherwise}
\end{cases} \\
\mathbf{X}_3 &= \begin{cases}
f_{\text{ta}}(\mathbf{X}_2) & \text{if } \alpha_{\text{ta}} = 1 \\
\mathbf{X}_2 & \text{otherwise}
\end{cases}
\end{align}
```

**è”åˆæŸå¤±å‡½æ•°**ï¼š
$$\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{CE}} + \lambda_1 \mathcal{L}_{\text{fs}} + \lambda_2 \mathcal{L}_{\text{ca}} + \lambda_3 \mathcal{L}_{\text{ta}}$$

å…¶ä¸­ï¼š
- $\mathcal{L}_{\text{CE}}$ï¼šäº¤å‰ç†µæŸå¤±
- $\mathcal{L}_{\text{fs}} = \|\mathbf{g}\|_1$ï¼šç‰¹å¾é€‰æ‹©ç¨€ç–æ€§æŸå¤±
- $\mathcal{L}_{\text{ca}} = \sum_{i,j} \|\mathbf{c}_{ij}\|_2^2$ï¼šç›¸å…³æ€§æ­£åˆ™åŒ–
- $\mathcal{L}_{\text{ta}} = \|\mathbf{A}\|_F^2$ï¼šæ³¨æ„åŠ›æƒé‡æ­£åˆ™åŒ–

#### æ¨¡å—é—´äº¤äº’åˆ†æ

**1. ç‰¹å¾é€‰æ‹©â†’ç›¸å…³æ„ŸçŸ¥**ï¼š
ç‰¹å¾é€‰æ‹©å‡å°‘å™ªå£°ï¼Œæé«˜ç›¸å…³æ€§è®¡ç®—çš„å‡†ç¡®æ€§ï¼š
$$\text{SNR}_{\text{improved}} = \frac{\text{Signal}_{\text{selected}}}{\text{Noise}_{\text{filtered}}}$$

**2. ç›¸å…³æ„ŸçŸ¥â†’æ—¶é—´æ³¨æ„åŠ›**ï¼š
ç»“æ„åŒ–ç‰¹å¾æä¾›æ›´å¥½çš„æ—¶é—´å»ºæ¨¡åŸºç¡€ï¼š
$$\text{Attention}_{\text{quality}} \propto \text{Feature}_{\text{structure}}$$

**3. ç«¯åˆ°ç«¯ä¼˜åŒ–**ï¼š
æ¢¯åº¦é€šè¿‡æ‰€æœ‰æ¨¡å—åå‘ä¼ æ’­ï¼š
$$\frac{\partial \mathcal{L}}{\partial \theta_{\text{fs}}} = \frac{\partial \mathcal{L}}{\partial \mathbf{X}_3} \cdot \frac{\partial \mathbf{X}_3}{\partial \mathbf{X}_2} \cdot \frac{\partial \mathbf{X}_2}{\partial \mathbf{X}_1} \cdot \frac{\partial \mathbf{X}_1}{\partial \theta_{\text{fs}}}$$

#### ç†è®ºä¼˜åŠ¿åˆ†æ

**1. äº’è¡¥æ€§**ï¼š
- ç‰¹å¾é€‰æ‹©ï¼šè§£å†³ç‰¹å¾å†—ä½™é—®é¢˜
- ç›¸å…³æ„ŸçŸ¥ï¼šåˆ©ç”¨ç‰©ç†ç»“æ„ä¿¡æ¯
- æ—¶é—´æ³¨æ„åŠ›ï¼šæ•è·é‡è¦æ—¶åºæ¨¡å¼

**2. é²æ£’æ€§**ï¼š
æ¨¡å—åŒ–è®¾è®¡æä¾›æ•…éšœå®¹é”™ï¼š
$$P(\text{System Failure}) = \prod_{i=1}^3 P(\text{Module}_i \text{ Failure})$$

**3. å¯æ‰©å±•æ€§**ï¼š
æ–°æ¨¡å—å¯æ— ç¼é›†æˆï¼š
$$f_{\text{new}} = f_{\text{module}_n} \circ f_{\text{module}_{n-1}} \circ ... \circ f_{\text{module}_1}$$

#### å…³é”®åˆ›æ–°ç‚¹
1. **ç»Ÿä¸€é›†æˆæ¡†æ¶**ï¼šä¸‰ç§äº’è¡¥æŠ€æœ¯çš„æœ‰æœºç»“åˆ
2. **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ”¯æŒåŠ¨æ€é…ç½®å’Œæ¶ˆèç ”ç©¶  
3. **ç«¯åˆ°ç«¯ä¼˜åŒ–**ï¼šæ‰€æœ‰ç»„ä»¶è”åˆè®­ç»ƒï¼Œé¿å…æ¬¡ä¼˜è§£
4. **ç‰©ç†çº¦æŸæ„ŸçŸ¥**ï¼šå°†ä¼ æ„Ÿå™¨ç‰©ç†çŸ¥è¯†èå…¥æ·±åº¦å­¦ä¹ 
5. **å¤šå±‚æ¬¡ç‰¹å¾å­¦ä¹ **ï¼šä»ç‰¹å¾çº§åˆ°æ—¶åºçº§çš„å±‚æ¬¡åŒ–å»ºæ¨¡

## å®éªŒæ–¹æ³•è®ºä¸ç†è®ºåˆ†æ

### æ•°æ®æ³„éœ²é˜²æ­¢ç­–ç•¥
**ç†è®ºåŸºç¡€**ï¼šä¼ ç»Ÿéšæœºåˆ’åˆ†ä¼šå¯¼è‡´åŒä¸€è¢«è¯•çš„æ•°æ®åˆ†å¸ƒåœ¨è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†ä¸­ï¼Œé€ æˆæ—¶é—´ä¾èµ–æ€§æ³„éœ²ã€‚

**è¢«è¯•çº§åˆ«åˆ†å‰²**ï¼š
$$\mathcal{S} = \{S_1, S_2, ..., S_{13}\} \rightarrow \{\mathcal{S}_{\text{train}}, \mathcal{S}_{\text{val}}, \mathcal{S}_{\text{test}}\}$$

**æ•°å­¦éªŒè¯**ï¼š
è®¾ $\mathcal{D}_{\text{train}} \cap \mathcal{D}_{\text{test}} = \emptyset$ åœ¨è¢«è¯•çº§åˆ«ï¼Œåˆ™ï¼š
$$P(\text{data leakage}) = P(\exists i,j : \text{subject}(x_i^{\text{train}}) = \text{subject}(x_j^{\text{test}})) = 0$$

### æ—¶é—´ä¾èµ–æ€§æ¶ˆé™¤
**é—®é¢˜å½¢å¼åŒ–**ï¼š
æ—¶é—´åºåˆ—ä¸­ç›¸é‚»çª—å£çš„ç›¸å…³æ€§ï¼š
$$\rho(W_i, W_{i+1}) = \frac{\text{Cov}(W_i, W_{i+1})}{\sigma(W_i)\sigma(W_{i+1})}$$

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. **éé‡å çª—å£**ï¼šæ­¥é•¿ $s = 2W$ï¼Œç¡®ä¿ $W_i \cap W_j = \emptyset$ for $|i-j| \geq 1$
2. **æ—¶é—´é¡ºåºæ‰“ä¹±**ï¼šéšæœºæ’åˆ—è®­ç»ƒçª—å£ï¼Œç ´é™¤æ—¶åºæ¨¡å¼
3. **çª—å£å†…ä¸€è‡´æ€§**ï¼šä»…ä¿ç•™æ ‡ç­¾å®Œå…¨ç›¸åŒçš„çª—å£

### è¿‡æ‹Ÿåˆé˜²æ­¢ç­–ç•¥
**ç†è®ºä¾æ®**ï¼šæ·±åº¦ç½‘ç»œå®¹æ˜“åœ¨é«˜ç»´ç¨€ç–æ•°æ®ä¸Šè¿‡æ‹Ÿåˆï¼Œéœ€è¦å¤šå±‚æ¬¡æ­£åˆ™åŒ–ã€‚

```python
æ­£åˆ™åŒ–æŠ€æœ¯ç»„åˆï¼š
â”œâ”€â”€ æ•°æ®å±‚é¢ï¼š
â”‚   â”œâ”€â”€ æ ‡ç­¾å¹³æ»‘ï¼šy_soft = (1-Îµ)y_hard + Îµ/K
â”‚   â”œâ”€â”€ è¾“å…¥æ‰°åŠ¨ï¼šx_aug = x + N(0, ÏƒÂ²)
â”‚   â””â”€â”€ æ—¶é—´çª—å£å¢å¼ºï¼šéšæœºèµ·å§‹ç‚¹é‡‡æ ·
â”œâ”€â”€ æ¨¡å‹å±‚é¢ï¼š
â”‚   â”œâ”€â”€ Dropoutï¼šp(x_i = 0) = p_drop
â”‚   â”œâ”€â”€ BatchNormï¼šx_norm = (x-Î¼)/Ïƒ
â”‚   â””â”€â”€ æƒé‡è¡°å‡ï¼šL2æ­£åˆ™åŒ– Î»||Î¸||Â²
â”œâ”€â”€ ä¼˜åŒ–å±‚é¢ï¼š
â”‚   â”œâ”€â”€ æ¢¯åº¦è£å‰ªï¼š||âˆ‡Î¸|| â‰¤ Ï„
â”‚   â”œâ”€â”€ å­¦ä¹ ç‡è°ƒåº¦ï¼šlr Ã— Î³ when plateau
â”‚   â””â”€â”€ æ—©åœï¼šmonitor val_loss patience
â””â”€â”€ æŸå¤±å±‚é¢ï¼š
    â”œâ”€â”€ æ ‡ç­¾å¹³æ»‘ï¼šå‡å°‘è¿‡åº¦è‡ªä¿¡
    â”œâ”€â”€ ç„¦ç‚¹æŸå¤±ï¼šå…³æ³¨å›°éš¾æ ·æœ¬
    â””â”€â”€ å¤šä»»åŠ¡å­¦ä¹ ï¼šç‰¹å¾çº§è¾…åŠ©æŸå¤±
```

**æ•°å­¦å»ºæ¨¡**ï¼š
æ€»æ­£åˆ™åŒ–æŸå¤±ï¼š
$$\mathcal{L}_{\text{reg}} = \lambda_1\|\theta\|_2^2 + \lambda_2\|\theta\|_1 + \lambda_3 H(\text{predictions})$$

å…¶ä¸­ $H(\cdot)$ ä¸ºé¢„æµ‹ç†µï¼Œé¼“åŠ±é€‚åº¦ä¸ç¡®å®šæ€§ã€‚

### è®­ç»ƒé…ç½®

### æ•°æ®å¤„ç†æµç¨‹
```python
é¢„å¤„ç†æµç¨‹ï¼š
â”œâ”€â”€ è¢«è¯•çº§åˆ«æ•°æ®åˆ†å‰² (é˜²æ­¢æ•°æ®æ³„éœ²)
â”œâ”€â”€ ç±»åˆ«å¹³è¡¡æ£€æŸ¥ (min_samples=5000)
â”œâ”€â”€ ç‰¹å¾æ ‡å‡†åŒ– (åŸºäºè®­ç»ƒé›†)
â”œâ”€â”€ æ—¶é—´çª—å£åˆ›å»º (window_size=20, non-overlapping)
â”œâ”€â”€ æ—¶é—´é¡ºåºæ‰“ä¹± (ç ´é™¤æ—¶é—´ä¾èµ–)
â””â”€â”€ æ‰¹é‡åŠ è½½ (batch_size=8)
```

## æ¶ˆèç ”ç©¶ (Ablation Study)

### ç ”ç©¶ç›®çš„
ç³»ç»Ÿæ€§è¯„ä¼°HybridNetä¸­å„ä¸ªç»„ä»¶å¯¹æ¨¡å‹æ€§èƒ½çš„è´¡çŒ®ï¼Œç†è§£ä¸åŒæŠ€æœ¯çš„ä½œç”¨æœºåˆ¶ã€‚

### æ¶ˆèé…ç½®
```python
æµ‹è¯•é…ç½®çŸ©é˜µï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ é…ç½®åç§°                â”‚ ç‰¹å¾é€‰æ‹© â”‚ ç›¸å…³æ„ŸçŸ¥ â”‚ æ—¶é—´æ³¨æ„åŠ› â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Baseline (No Components)â”‚    âŒ    â”‚    âŒ    â”‚     âŒ     â”‚
â”‚ Feature Selection Only  â”‚    âœ…    â”‚    âŒ    â”‚     âŒ     â”‚
â”‚ Correlation Aware Only  â”‚    âŒ    â”‚    âœ…    â”‚     âŒ     â”‚
â”‚ Temporal Attention Only â”‚    âŒ    â”‚    âŒ    â”‚     âœ…     â”‚
â”‚ Feature + Correlation   â”‚    âœ…    â”‚    âœ…    â”‚     âŒ     â”‚
â”‚ Feature + Attention     â”‚    âœ…    â”‚    âŒ    â”‚     âœ…     â”‚
â”‚ Correlation + Attention â”‚    âŒ    â”‚    âœ…    â”‚     âœ…     â”‚
â”‚ Full HybridNet          â”‚    âœ…    â”‚    âœ…    â”‚     âœ…     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### è¯„ä¼°æŒ‡æ ‡
- **æ€§èƒ½æŒ‡æ ‡**ï¼šæµ‹è¯•å‡†ç¡®ç‡ã€F1åˆ†æ•°
- **æ•ˆç‡æŒ‡æ ‡**ï¼šè®­ç»ƒæ—¶é—´ã€æ”¶æ•›è½®æ•°
- **è´¡çŒ®åˆ†æ**ï¼šç›¸å¯¹äºåŸºçº¿çš„æ€§èƒ½æå‡

### å¯è§†åŒ–è¾“å‡º
- æ€§èƒ½æ’åå›¾è¡¨
- ç»„ä»¶è´¡çŒ®çƒ­åŠ›å›¾
- å¤æ‚åº¦vsæ€§èƒ½æ•£ç‚¹å›¾
- è®­ç»ƒæ•ˆç‡å¯¹æ¯”

## ä½¿ç”¨æ–¹æ³•

### ç¯å¢ƒè¦æ±‚
```bash
pip install torch torchvision
pip install scikit-learn pandas numpy
pip install matplotlib seaborn
```

### å¿«é€Ÿå¼€å§‹
```bash
# 1. è¿è¡Œå®Œæ•´å®éªŒ
cd code
python run.py

# 2. æŸ¥çœ‹æ—¥å¿—
tail -f ../logs/experiment_log_*.txt

# 3. æŸ¥çœ‹ç»“æœ
ls ../results/
```

### é…ç½®é€‰é¡¹
```python
CONFIG = {
    'min_samples_per_class': 5000,  # æ¯ç±»æœ€å°æ ·æœ¬æ•°
    'max_files': 253,               # æœ€å¤§æ–‡ä»¶æ•°
    'include_ablation': True,       # æ˜¯å¦åŒ…å«æ¶ˆèç ”ç©¶
}
```

## è®¡ç®—å¤æ‚åº¦åˆ†æ

### æ¨¡å‹å¤æ‚åº¦å¯¹æ¯”
**å‚æ•°æ•°é‡åˆ†æ**ï¼š
```python
æ¨¡å‹å‚æ•°ç»Ÿè®¡ï¼š
â”œâ”€â”€ Baseline CNN-LSTM:      ~2.1M å‚æ•°
â”œâ”€â”€ Correlation-Aware CNN:  ~1.8M å‚æ•° (åˆ†ç»„å·ç§¯å‡å°‘)
â”œâ”€â”€ Attention LSTM:         ~3.2M å‚æ•° (æ³¨æ„åŠ›æœºåˆ¶å¢åŠ )
â”œâ”€â”€ Feature-Selective Net:  ~2.3M å‚æ•° (é—¨æ§ç½‘ç»œå¼€é”€)
â””â”€â”€ HybridNet:             ~4.1M å‚æ•° (é›†æˆæ¶æ„)
```

**æ—¶é—´å¤æ‚åº¦**ï¼š
è®¾è¾“å…¥ç»´åº¦ä¸º $d=70$ï¼Œåºåˆ—é•¿åº¦ä¸º $T=20$ï¼Œéšè—ç»´åº¦ä¸º $h=128$

| æ¨¡å‹ | è®­ç»ƒå¤æ‚åº¦ | æ¨ç†å¤æ‚åº¦ | ä¸»è¦ç“¶é¢ˆ |
|------|-----------|-----------|----------|
| Baseline | $O(Th^2 + Td^2)$ | $O(Th^2)$ | LSTMè®¡ç®— |
| Correlation-Aware | $O(Td^2/G)$ | $O(Td^2/G)$ | åˆ†ç»„å·ç§¯ |
| Attention | $O(T^2h + Th^2)$ | $O(T^2h)$ | æ³¨æ„åŠ›çŸ©é˜µ |
| Feature-Selective | $O(Td^2 + d^2)$ | $O(Td^2)$ | é—¨æ§è®¡ç®— |
| HybridNet | $O(T^2h + Td^2)$ | $O(T^2h)$ | ç»¼åˆå¤æ‚åº¦ |

**ç©ºé—´å¤æ‚åº¦**ï¼š
- ç‰¹å¾å­˜å‚¨ï¼š$O(BTd)$ where $B$ = batch size
- ä¸­é—´æ¿€æ´»ï¼šå„æ¨¡å‹ä¸åŒçš„å†…å­˜å ç”¨æ¨¡å¼
- æ¢¯åº¦å­˜å‚¨ï¼šä¸å‚æ•°æ•°é‡çº¿æ€§ç›¸å…³

### ç†è®ºè´¡çŒ®ä¸åˆ›æ–°ç‚¹

#### 1. ç‰©ç†çº¦æŸæ·±åº¦å­¦ä¹ æ¡†æ¶
**è´¡çŒ®**ï¼šé¦–æ¬¡å°†ä¼ æ„Ÿå™¨ç‰©ç†ç‰¹æ€§ç³»ç»Ÿæ€§åœ°èå…¥æ·±åº¦å­¦ä¹ æ¶æ„
**åˆ›æ–°ç‚¹**ï¼š
- åŸºäºå››å…ƒæ•°ã€é€Ÿåº¦ã€ç£åœºçš„ç‰©ç†åˆ†ç»„ç­–ç•¥
- ç»„å†…å¼ºåŒ–å­¦ä¹ ä¸ç»„é—´ç›¸å…³æ€§å»ºæ¨¡
- ç‰©ç†çº¦æŸä¸‹çš„ç‰¹å¾è¡¨ç¤ºå­¦ä¹ 

**ç†è®ºæ„ä¹‰**ï¼š
$$\text{Physical Constraint} + \text{Deep Learning} \rightarrow \text{Physics-Informed Neural Networks}$$

#### 2. å¤šæ¨¡æ€æ—¶åºç‰¹å¾é€‰æ‹©
**è´¡çŒ®**ï¼šåŠ¨æ€ç‰¹å¾é€‰æ‹©åœ¨æ—¶åºåˆ†ç±»ä¸­çš„é¦–æ¬¡åº”ç”¨
**æ•°å­¦æ¡†æ¶**ï¼š
$$\mathbf{X}_{\text{selected}} = \mathbf{G}(\mathbf{X}) \odot \mathbf{X}$$
å…¶ä¸­ $\mathbf{G}: \mathbb{R}^{T \times d} \rightarrow [0,1]^d$ ä¸ºå¯å­¦ä¹ é—¨æ§å‡½æ•°

**ç†è®ºåˆ†æ**ï¼š
- ä¿¡æ¯ç“¶é¢ˆç†è®ºï¼šæœ€å¤§åŒ–ç›¸å…³ä¿¡æ¯ï¼Œæœ€å°åŒ–å†—ä½™ä¿¡æ¯
- ç¨€ç–æ€§ç†è®ºï¼šé€šè¿‡L1æ­£åˆ™åŒ–å®ç°è‡ªåŠ¨ç‰¹å¾é€‰æ‹©
- å¯è§£é‡Šæ€§ï¼šé—¨æ§æƒé‡æä¾›ç‰¹å¾é‡è¦æ€§ç›´è§‚è§£é‡Š

#### 3. å±‚æ¬¡åŒ–æ—¶åºæ³¨æ„åŠ›æœºåˆ¶
**è´¡çŒ®**ï¼šå°†Transformeræ³¨æ„åŠ›æœºåˆ¶é€‚é…åˆ°ä¼ æ„Ÿå™¨æ—¶åºæ•°æ®
**åˆ›æ–°ç‚¹**ï¼š
- å±€éƒ¨LSTMç¼–ç  + å…¨å±€è‡ªæ³¨æ„åŠ›
- å¤šå¤´æœºåˆ¶æ•è·ä¸åŒæ—¶é—´æ¨¡å¼
- æ®‹å·®è¿æ¥ä¿è¯æ·±å±‚ç½‘ç»œè®­ç»ƒç¨³å®šæ€§

**æ•°å­¦å»ºæ¨¡**ï¼š
$$\text{Attention}_{multi} = \text{Concat}_{i=1}^h \text{Attention}_{head_i}$$

#### 4. ç«¯åˆ°ç«¯é›†æˆå­¦ä¹ èŒƒå¼
**è´¡çŒ®**ï¼šæå‡ºæ¨¡å—åŒ–å¯é…ç½®çš„æ·±åº¦å­¦ä¹ é›†æˆæ¡†æ¶
**ç†è®ºåŸºç¡€**ï¼š
- é›†æˆå­¦ä¹ ç†è®ºï¼šå¤šä¸ªå¼±å­¦ä¹ å™¨ç»„åˆæˆå¼ºå­¦ä¹ å™¨
- æ¨¡å—åŒ–è®¾è®¡ï¼šæ”¯æŒç»„ä»¶çº§æ¶ˆèå’Œåˆ†æ
- è”åˆä¼˜åŒ–ï¼šé¿å…è´ªå¿ƒé›†æˆçš„æ¬¡ä¼˜è§£

**æ•°å­¦è¡¨ç¤º**ï¼š
$$f_{\text{ensemble}} = f_{\text{attention}} \circ f_{\text{correlation}} \circ f_{\text{selection}}$$

### å®éªŒè®¾è®¡çš„ç§‘å­¦æ€§
**æ§åˆ¶å˜é‡åŸåˆ™**ï¼š
- ç›¸åŒæ•°æ®é›†ã€ç›¸åŒé¢„å¤„ç†ã€ç›¸åŒè¯„ä¼°æŒ‡æ ‡
- ç›¸åŒè¶…å‚æ•°è°ƒä¼˜ç­–ç•¥å’Œè®¡ç®—èµ„æº
- ç›¸åŒéšæœºç§å­ç¡®ä¿ç»“æœå¯é‡ç°

**ç»Ÿè®¡æ˜¾è‘—æ€§**ï¼š
- å¤šæ¬¡è¿è¡Œå–å¹³å‡å€¼å’Œæ ‡å‡†å·®
- é…å¯¹tæ£€éªŒéªŒè¯æ€§èƒ½å·®å¼‚æ˜¾è‘—æ€§
- ç½®ä¿¡åŒºé—´ä¼°è®¡å’Œæ•ˆåº”é‡è®¡ç®—

**æ¶ˆèç ”ç©¶è®¾è®¡**ï¼š
$$2^3 = 8 \text{ ç§é…ç½®ç»„åˆï¼Œç³»ç»Ÿæ€§åˆ†ææ¯ä¸ªç»„ä»¶çš„è´¡çŒ®}$$

## è¾“å‡ºç»“æœ

### è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶
```
../results/
â”œâ”€â”€ pytorch_experimental_results.csv      # æ¨¡å‹æ€§èƒ½å¯¹æ¯”è¡¨
â”œâ”€â”€ detailed_experimental_results.json    # è¯¦ç»†ç»“æœJSON
â”œâ”€â”€ comprehensive_results.png             # ç»¼åˆæ€§èƒ½å¯è§†åŒ–
â”œâ”€â”€ training_curves.png                   # è®­ç»ƒæ›²çº¿å›¾
â”œâ”€â”€ all_confusion_matrices.png            # æ‰€æœ‰æ··æ·†çŸ©é˜µ
â”œâ”€â”€ training_histories/                   # è®­ç»ƒå†å²
â”‚   â”œâ”€â”€ Baseline_CNN-LSTM_history.json
â”‚   â”œâ”€â”€ Attention_LSTM_history.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ confusion_matrices/                   # ä¸ªåˆ«æ··æ·†çŸ©é˜µ
â”œâ”€â”€ roc_curves/                          # ROCæ›²çº¿å›¾
â””â”€â”€ ablation_study/                      # æ¶ˆèç ”ç©¶ç»“æœ
    â”œâ”€â”€ ablation_results.json
    â”œâ”€â”€ ablation_summary.csv
    â””â”€â”€ ablation_visualization.png
```

### æ€§èƒ½è¯„ä¼°ä¸è§£é‡Š
```python
è¯„ä¼°æŒ‡æ ‡ï¼š
â”œâ”€â”€ å‡†ç¡®ç‡ (Accuracy)
â”œâ”€â”€ F1åˆ†æ•° (F1-Score) 
â”œâ”€â”€ ç²¾ç¡®ç‡ (Precision)
â”œâ”€â”€ å¬å›ç‡ (Recall)
â”œâ”€â”€ è®­ç»ƒæ—¶é—´ (Training Time)
â”œâ”€â”€ è¿‡æ‹Ÿåˆåˆ†æ (Train-Val Gap)
â””â”€â”€ ROC-AUC (å¤šç±»åˆ«)
```

## å®éªŒç»“æœè§£é‡Š

### æ€§èƒ½æŒ‡æ ‡è¯´æ˜
- **Train-Val Gap < 0.2**ï¼šæ¨¡å‹æ³›åŒ–è‰¯å¥½
- **Train-Val Gap > 0.2**ï¼šå­˜åœ¨è¿‡æ‹Ÿåˆé£é™©
- **Val Accuracy > 80%**ï¼šä¼˜ç§€æ€§èƒ½
- **Val Accuracy 60-80%**ï¼šè‰¯å¥½æ€§èƒ½
- **Val Accuracy < 60%**ï¼šéœ€è¦æ”¹è¿›

### å¸¸è§é—®é¢˜æ’æŸ¥
1. **è¿‡æ‹Ÿåˆ**ï¼šè®­ç»ƒå‡†ç¡®ç‡è¿œé«˜äºéªŒè¯å‡†ç¡®ç‡
   - è§£å†³ï¼šå¢åŠ æ­£åˆ™åŒ–ã€å‡å°‘æ¨¡å‹å¤æ‚åº¦
2. **æ¬ æ‹Ÿåˆ**ï¼šè®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡éƒ½å¾ˆä½
   - è§£å†³ï¼šå¢åŠ æ¨¡å‹å®¹é‡ã€è°ƒæ•´å­¦ä¹ ç‡
3. **ç±»åˆ«ä¸å¹³è¡¡**ï¼šæŸäº›ç±»åˆ«è¯†åˆ«ç‡å¾ˆä½
   - è§£å†³ï¼šç±»åˆ«æƒé‡å¹³è¡¡ã€æ•°æ®å¢å¼º

## æŠ€æœ¯ç‰¹ç‚¹

### åˆ›æ–°ç‚¹
1. **å¤šæŠ€æœ¯èåˆ**ï¼šé¦–æ¬¡å°†ç‰¹å¾é€‰æ‹©ã€ç›¸å…³æ„ŸçŸ¥å’Œæ—¶é—´æ³¨æ„åŠ›ç»“åˆ
2. **è¿‡æ‹Ÿåˆé˜²æŠ¤**ï¼šå…¨é¢çš„æ­£åˆ™åŒ–ç­–ç•¥ç¡®ä¿æ¨¡å‹æ³›åŒ–
3. **ç³»ç»Ÿæ¶ˆè**ï¼šè¯¦ç»†åˆ†æå„ç»„ä»¶è´¡çŒ®åº¦
4. **å®ç”¨æ€§å¼º**ï¼šçœŸå®æŠ¤ç†æ•°æ®éªŒè¯ï¼Œå¯éƒ¨ç½²åº”ç”¨

### åº”ç”¨ä»·å€¼
- **æ™ºèƒ½æŠ¤ç†**ï¼šè‡ªåŠ¨è¯†åˆ«æŠ¤ç†æ´»åŠ¨ï¼Œæé«˜æŠ¤ç†è´¨é‡
- **å¥åº·ç›‘æµ‹**ï¼šå®æ—¶æ´»åŠ¨ç›‘æµ‹ï¼ŒåŠæ—¶å‘ç°å¼‚å¸¸
- **ç ”ç©¶å·¥å…·**ï¼šä¸ºæŠ¤ç†ç ”ç©¶æä¾›å®¢è§‚æ•°æ®æ”¯æŒ

---

# Comprehensive Technical Documentation

## Advanced Model Architecture Analysis

### Detailed Mathematical Formulations

#### 1. Baseline CNN-LSTM Mathematical Framework

**Convolutional Feature Extraction**:
$$\mathbf{h}^{conv} = \sigma(\mathbf{W}^{conv} * \mathbf{X} + \mathbf{b}^{conv})$$

where $*$ denotes convolution operation, $\mathbf{W}^{conv} \in \mathbb{R}^{k \times D \times F}$ are learnable filters with kernel size $k$ and $F$ output channels.

**LSTM Temporal Processing**:
$$\mathbf{f}_t = \sigma(\mathbf{W}_f \cdot [\mathbf{h}_t^{conv}, \mathbf{h}_{t-1}] + \mathbf{b}_f)$$
$$\mathbf{i}_t = \sigma(\mathbf{W}_i \cdot [\mathbf{h}_t^{conv}, \mathbf{h}_{t-1}] + \mathbf{b}_i)$$
$$\tilde{\mathbf{C}}_t = \tanh(\mathbf{W}_C \cdot [\mathbf{h}_t^{conv}, \mathbf{h}_{t-1}] + \mathbf{b}_C)$$
$$\mathbf{C}_t = \mathbf{f}_t \odot \mathbf{C}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{C}}_t$$
$$\mathbf{o}_t = \sigma(\mathbf{W}_o \cdot [\mathbf{h}_t^{conv}, \mathbf{h}_{t-1}] + \mathbf{b}_o)$$
$$\mathbf{h}_t = \mathbf{o}_t \odot \tanh(\mathbf{C}_t)$$

#### 2. Correlation-Aware CNN Mathematical Framework

**Feature Correlation Matrix**:
$$\mathbf{R} = \frac{1}{T-1} \sum_{t=1}^{T} (\mathbf{x}_t - \boldsymbol{\mu})(\mathbf{x}_t - \boldsymbol{\mu})^T$$

**Correlation-Aware Convolution**:
$$\mathbf{h}^{corr}_t = \sigma(\mathbf{W}^{corr} \cdot [\mathbf{x}_t, \mathbf{R} \mathbf{x}_t] + \mathbf{b}^{corr})$$

**Adaptive Feature Weighting**:
$$\boldsymbol{\alpha} = \text{softmax}(\mathbf{W}_{\alpha} \text{vec}(\mathbf{R}) + \mathbf{b}_{\alpha})$$
$$\mathbf{h}^{weighted} = \boldsymbol{\alpha} \odot \mathbf{h}^{corr}$$

#### 3. Attention LSTM Mathematical Framework

**LSTM Encoding**:
$$\mathbf{h}_t = \text{LSTM}(\mathbf{x}_t, \mathbf{h}_{t-1})$$

**Attention Mechanism**:
$$e_{t,i} = \mathbf{v}_a^T \tanh(\mathbf{W}_a \mathbf{h}_t + \mathbf{U}_a \mathbf{h}_i + \mathbf{b}_a)$$
$$\alpha_{t,i} = \frac{\exp(e_{t,i})}{\sum_{j=1}^{T} \exp(e_{t,j})}$$

**Context Vector**:
$$\mathbf{c}_t = \sum_{i=1}^{T} \alpha_{t,i} \mathbf{h}_i$$

#### 4. Feature-Selective Net Mathematical Framework

**Feature Importance Scoring**:
$$\mathbf{s} = \sigma(\mathbf{W}_s \mathbf{X} + \mathbf{b}_s)$$

**Gating Mechanism**:
$$\mathbf{g} = \text{sigmoid}(\mathbf{W}_g \mathbf{s} + \mathbf{b}_g)$$

**Feature Selection**:
$$\mathbf{X}^{selected} = \mathbf{g} \odot \mathbf{X}$$

#### 5. HybridNet Mathematical Framework

**Feature Selection Module**:
$$\mathbf{g}_{fs} = \text{sigmoid}(\mathbf{W}_{fs} \tanh(\mathbf{W}_{fs}' \mathbf{X} + \mathbf{b}_{fs}') + \mathbf{b}_{fs})$$
$$\mathbf{X}_{fs} = \mathbf{g}_{fs} \odot \mathbf{X}$$

**Correlation-Aware Processing**:
$$\mathbf{R}_t = \text{BatchCorr}(\mathbf{X}_{fs})$$
$$\mathbf{h}_{ca} = \text{Conv1D}([\mathbf{X}_{fs}, \mathbf{R}_t \mathbf{X}_{fs}])$$

**Temporal Attention**:
$$\mathbf{h}_{lstm} = \text{BiLSTM}(\mathbf{h}_{ca})$$
$$e_t = \mathbf{v}^T \tanh(\mathbf{W}_e \mathbf{h}_{lstm,t} + \mathbf{b}_e)$$
$$\alpha_t = \frac{\exp(e_t)}{\sum_{j=1}^{T} \exp(e_j)}$$
$$\mathbf{c} = \sum_{t=1}^{T} \alpha_t \mathbf{h}_{lstm,t}$$

**Multi-Scale Fusion**:
$$\mathbf{h}_{final} = \mathbf{W}_{fusion} [\mathbf{c}; \mathbf{h}_{lstm,T}; \text{GlobalAvgPool}(\mathbf{h}_{ca})] + \mathbf{b}_{fusion}$$

**Classification**:
$$\mathbf{y} = \text{softmax}(\mathbf{W}_{clf} \mathbf{h}_{final} + \mathbf{b}_{clf})$$

## Comprehensive Dataset Analysis and Statistics

### SONaR Dataset Enhanced Statistical Analysis

The dataset analysis tools provide detailed insights into subject behavior, activity patterns, and temporal characteristics:

#### Statistical Analysis Features
- **Per-Subject Statistics**: Individual analysis for each subject including sample counts, activity distribution, and feature statistics
- **Per-Activity Statistics**: Detailed metrics for each nursing activity including duration, participation, and feature characteristics
- **Advanced Statistical Measures**: Mean, standard deviation, quartiles, skewness, kurtosis, IQR, and range for all numeric features
- **Correlation Analysis**: Feature correlation matrix with identification of highly correlated pairs (>0.8)

#### Temporal Pattern Analysis
- **Sampling Rate Analysis**: Detailed sampling frequency statistics with consistency measures
- **Activity Duration Analysis**: Comprehensive temporal patterns including short/medium/long activity categorization
- **Time-based Statistics**: Interval consistency, sampling rate distribution, and temporal quality metrics
- **Subject Temporal Profiles**: Individual temporal characteristics per subject

#### Dataset Characteristics
- **Total samples**: 7,631,843 temporal measurements
- **Subjects**: 14 healthcare professionals
- **Activity classes**: 20 nursing activities
- **Feature dimensionality**: 70 sensor measurements
- **Window size**: 20 timesteps
- **Sampling frequency**: Variable (preserved from original data)

#### Class Distribution Analysis
The dataset exhibits significant class imbalance with ratio 156.72:1 between most and least frequent activities. This motivated our use of weighted loss functions and balanced sampling strategies.

## Ablation Study Discussion and Analysis

### Executive Summary

This section presents a comprehensive analysis of the systematic ablation study conducted on the HybridNet architecture for nursing activity recognition. The study evaluated eight distinct configurations, ranging from individual components to the complete integrated model, using the SONaR dataset.

### Experimental Design and Methodology

#### Study Configuration

The ablation study was designed to systematically evaluate three core architectural components: 
1. Adaptive feature selection mechanism
2. Correlation-aware processing with physical sensor grouping
3. Temporal attention mechanism

Eight configurations were tested using identical hyperparameters, data splits, and training protocols to ensure fair comparison. Each model was trained for up to 200 epochs with early stopping based on validation accuracy, using a patience threshold of 100 epochs.

#### Training Infrastructure and Optimization

All models were trained using CUDA-accelerated PyTorch implementation with comprehensive regularization strategies including:
- Label smoothing (Îµ=0.1)
- Weight decay (1Ã—10â»â´)
- Dropout and gradient clipping
- AdamW optimizer with initial learning rate of 1Ã—10â»â´
- ReduceLROnPlateau scheduling
- Batch sizes set to 8 to accommodate GPU memory constraints

### Performance Analysis

#### Baseline Configuration Performance

The baseline configuration achieved:
- **Test accuracy**: 68.32%
- **F1-score**: 67.99%
- **Precision**: 69.54%
- **Recall**: 68.32%

The model exhibited severe overfitting with a train-validation accuracy gap of 45.18% (78.45% training vs 33.27% validation accuracy).

#### Single Component Analysis

**Feature Selection Component** (Test Metrics: 69.33% Accuracy, 69.34% F1-Score):
- Modest but consistent improvements over baseline (+1.01% accuracy, +1.35% F1-score)
- Strong alignment between accuracy and F1-score indicates balanced performance across activity classes

**Temporal Attention Mechanism** (Test Metrics: 78.33% Accuracy, 78.66% F1-Score):
- Strongest individual performance with substantial improvements over baseline (+10.01% accuracy, +10.67% F1-score)
- Superior F1-score relative to accuracy indicates particularly strong performance on minority classes
- High precision (80.82%) demonstrates the attention mechanism's ability to make confident, accurate predictions

### Component Interaction Analysis

#### Synergistic vs. Antagonistic Interactions

The ablation study reveals both synergistic and antagonistic interactions between components:
- **Positive synergy**: Feature selection and temporal attention combination showed enhanced effectiveness
- **Negative interactions**: Most other combinations showed performance degradation below individual components
- **Competition for capacity**: Correlation-aware component's grouping strategy may conflict with feature selection mechanism's learned importance patterns

### Implications for Nursing Activity Recognition

#### Architectural Design Insights

The ablation study results provide crucial insights:
- Temporal attention mechanism emerges as the most valuable component
- Simple architectures may be more effective than complex combinations for this domain
- Severe overfitting in high-performing models suggests need for larger datasets or better regularization

## Supplementary Architecture Details

### Comprehensive Model Comparison Matrix

| Model | Primary Innovation | Key Components | Computational Cost | Training Complexity | Interpretability |
|-------|-------------------|----------------|-------------------|-------------------|------------------|
| **Baseline CNN-LSTM** | Standard approach | Conv1D + LSTM | O(TÂ·DÂ·H + TÂ·HÂ²) | Low | Medium |
| **Correlation-Aware CNN** | Inter-sensor correlation | Correlation matrix + Adaptive weighting | O(TÂ·DÂ² + TÂ·DÂ·H) | Medium | High |
| **Attention LSTM** | Temporal focus | Self-attention + BiLSTM | O(TÂ²Â·H + TÂ·HÂ²) | Medium | High |
| **Feature-Selective Net** | Automatic feature selection | Gating mechanism + CNN-LSTM | O(TÂ·DÂ·H + TÂ·HÂ²) | Low | High |
| **HybridNet** | Multi-mechanism integration | All above components | O(TÂ·DÂ² + TÂ²Â·H + TÂ·HÂ²) | High | Very High |

### Data Flow Architecture

#### HybridNet Data Flow Diagram

```
Input Sequence X âˆˆ â„^(TÃ—D)
           â†“
    [Preprocessing]
     - Z-score normalization
     - Window segmentation (W=20)
           â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Feature Selection   â”‚
  â”‚  Module (FSM)       â”‚
  â”‚  g_fs = Ïƒ(W_fsÂ·X)   â”‚
  â”‚  X_fs = g_fs âŠ™ X    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Correlation-Aware   â”‚
  â”‚ Processing (CAP)    â”‚
  â”‚ R_t = BatchCorr(X_fs)â”‚
  â”‚ h_ca = Conv1D([X_fs,â”‚
  â”‚              R_tÂ·X_fs])â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Temporal Attention â”‚
  â”‚  Mechanism (TAM)    â”‚
  â”‚  h_lstm = BiLSTM(h_ca)â”‚
  â”‚  Î±_t = softmax(e_t) â”‚
  â”‚  c = Î£ Î±_tÂ·h_lstm,t â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Multi-Scale       â”‚
  â”‚   Fusion (MSF)      â”‚
  â”‚ h_final = W_fusion  â”‚
  â”‚ [c; h_T; GlobalAvg] â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Classification    â”‚
  â”‚      Head           â”‚
  â”‚ y = softmax(WÂ·h+b)  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
    Output Probabilities
```

### Mathematical Derivations Extended

#### Feature Selection Mechanism Derivation

The feature selection module learns importance weights for each feature dimension:

1. **First Layer Transformation**:
   $$\mathbf{h}_1 = \tanh(\mathbf{W}_1 \mathbf{X} + \mathbf{b}_1)$$
   where $\mathbf{W}_1 \in \mathbb{R}^{D/2 \times D}$ reduces dimensionality for computational efficiency.

2. **Importance Score Generation**:
   $$\mathbf{s} = \mathbf{W}_2 \mathbf{h}_1 + \mathbf{b}_2$$
   where $\mathbf{W}_2 \in \mathbb{R}^{D \times D/2}$ maps back to original dimension.

3. **Gating Function**:
   $$\mathbf{g} = \sigma(\mathbf{s})$$
   The sigmoid ensures gates are in [0,1], allowing soft selection.

4. **Feature Selection**:
   $$\mathbf{X}_{selected} = \mathbf{g} \odot \mathbf{X}$$
   Element-wise multiplication applies learned importance weights.

## Training Methodology Extended

### Optimization Strategy

**Loss Function**: Cross-entropy with label smoothing to improve generalization:
$$\mathcal{L} = -\sum_{i=1}^{N} \sum_{c=1}^{C} y_{i,c}^{smooth} \log(\hat{y}_{i,c})$$

where $y_{i,c}^{smooth} = (1-\epsilon)y_{i,c} + \frac{\epsilon}{C}$ with smoothing parameter $\epsilon = 0.1$.

**Optimizer**: AdamW with weight decay:
$$\theta_{t+1} = \theta_t - \eta (\nabla_\theta \mathcal{L} + \lambda \theta_t)$$

### Hyperparameters
- Learning rate: $\eta = 10^{-4}$
- Weight decay: $\lambda = 10^{-4}$
- Batch size: 8 (memory-optimized)
- Maximum epochs: 200
- Early stopping patience: 100

### Regularization Techniques
1. **Dropout**: Applied with rate 0.3 in fully connected layers
2. **Gradient Clipping**: Max norm of 1.0 to prevent exploding gradients
3. **Learning Rate Scheduling**: ReduceLROnPlateau with factor 0.5

### Data Splitting Strategy

**Subject-based Stratification**: To prevent data leakage and ensure generalizability:
- Training: 60% of subjects
- Validation: 20% of subjects  
- Testing: 20% of subjects

**Temporal Window Creation**: Non-overlapping windows to preserve independence:
$$\text{Windows} = \{[\mathbf{x}_{i}, \mathbf{x}_{i+W-1}] : i = 0, W, 2W, ...\}$$

## Implementation Details

### Framework
- **Deep Learning**: PyTorch 1.x
- **Optimization**: CUDA-accelerated training when available
- **Memory Management**: Gradient accumulation and cache clearing

### Reproducibility
- **Random Seeds**: Fixed across all experiments
- **Data Splits**: Deterministic subject-based stratification
- **Model Initialization**: Xavier/He initialization schemes

### Computational Complexity

#### Time Complexity
- **Baseline CNN-LSTM**: $O(T \cdot D \cdot H + T \cdot H^2)$
- **HybridNet**: $O(T \cdot D^2 + T \cdot D \cdot H + T \cdot H^2)$

where $T$ is sequence length, $D$ is feature dimension, and $H$ is hidden dimension.

#### Space Complexity
All models: $O(T \cdot D + H^2 + C \cdot H)$ for parameters and activations.

## Conclusions and Future Work

### Key Findings

This comprehensive analysis reveals complex interactions between architectural components in nursing activity recognition that challenge simple assumptions about component additivity. While individual components can provide substantial benefits, their combination often results in negative interactions that severely degrade performance.

Key findings include:
- **Temporal attention mechanism** emerges as the most valuable component
- **Simple architectures** may be more effective than complex combinations
- **Severe overfitting** in high-performing models suggests need for larger datasets

### Architectural Design Insights

The findings have important implications for both research and practical deployment:
- Simple, well-designed architectures focused on temporal modeling appear more effective
- Current datasets may be insufficient for training highly complex architectures
- Need for larger, more diverse training datasets or more sophisticated regularization strategies

### Future Research Directions

#### Component Redesign Opportunities
- Alternative correlation modeling approaches that maintain flexibility
- Modified attention mechanisms that account for feature selection effects
- Hierarchical integration and learned component weighting strategies

#### Dataset and Evaluation Improvements
- Balanced sampling strategies and synthetic data augmentation
- Alternative evaluation metrics that better reflect real-world deployment scenarios
- Larger datasets with more subjects for stable training of complex architectures

### Model Selection Justification

**Baseline CNN-LSTM**: Established benchmark for time-series classification, providing reliable comparison baseline.

**Correlation-Aware CNN**: Nursing activities involve coordinated movements across multiple sensors; explicit correlation modeling captures these inter-dependencies.

**Attention LSTM**: Variable-duration activities require temporal focus mechanisms to identify critical execution phases.

**Feature-Selective Net**: High-dimensional sensor data contains noise; learnable selection improves signal-to-noise ratio.

**HybridNet**: Integrates proven mechanisms to address multiple challenges simultaneously: noise reduction, correlation modeling, and temporal focus.

### Final Recommendations

These results underscore the importance of systematic ablation studies in architecture design, revealing that theoretical advantages of individual components do not necessarily translate to improved performance when combined. Future research should prioritize understanding and mitigating negative component interactions rather than simply adding more architectural complexity.

The comprehensive documentation provides empirical evidence for the effectiveness of specialized components in nursing activity recognition. The proposed HybridNet architecture demonstrates the benefits of integrating multiple mechanisms to address complex challenges while highlighting the importance of careful architectural design and thorough experimental evaluation.

---

## å¼•ç”¨

å¦‚æœæ‚¨ä½¿ç”¨äº†æœ¬é¡¹ç›®çš„ä»£ç æˆ–æ–¹æ³•ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{nursing_activity_recognition,
  title={Deep Learning Models for Nursing Activity Recognition: A Comprehensive Analysis},
  author={Your Name},
  year={2024},
  url={https://github.com/your-repo}
}
```

## è®¸å¯è¯

MIT License - è¯¦è§ LICENSE æ–‡ä»¶

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·è”ç³»ï¼š[your-email@example.com]

---

*This comprehensive documentation serves as a complete reference for the deep learning models and methodologies developed for nursing activity recognition, providing both theoretical foundations and practical implementation details for researchers and practitioners in the field.*

**æœ€åæ›´æ–°**: 2024å¹´6æœˆ27æ—¥ 